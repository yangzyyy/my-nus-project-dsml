{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we visualize, via contour plots, a randomly-initialized neural network as a function of its inputs, and investigate its dependence on depth.\n",
    "\n",
    "We current do not have a complete understanding as to why or when deep neural networks have an advantage over shallow ones, but this is an active area of research. Some references on this topic are given below.\n",
    "\n",
    "    (1) Lu, Z.; Pu, H.; Wang, F.; Hu, Z.; Wang, L. The Expressive Power of Neural Networks: A View from the Width. In Advances in neural information processing systems; 2017; pp 6231–6239.\n",
    "    (2) Shen, Z.; Yang, H.; Zhang, S. Nonlinear Approximation via Compositions. arXiv preprint arXiv:1902.10170 2019.\n",
    "    (3) Daubechies, I.; DeVore, R.; Foucart, S.; Hanin, B.; Petrova, G. Nonlinear Approximation and (Deep) ReLU Networks. arXiv:1905.02199 [cs] 2019.\n",
    "    (4) Shen, Z.; Yang, H.; Zhang, S. Deep Network Approximation Characterized by Number of Neurons. 2019.\n",
    "    (5) Daws, J.; Webster, C. Analysis of Deep Neural Networks with Quasi-Optimal Polynomial Approximation Rates. arXiv:1912.02302 [cs, math] 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sns.set(font_scale=1.5)\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Produce Contour Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will write a handy function to plot a 2D contour map of a given function $f$. This can be easily done by the `plt.contourf` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(f, x_range=(-10, 10), y_range=(-10, 10), delta=0.1, **contour_kwargs):\n",
    "    \"\"\"\n",
    "    Plots 2D contour of f\n",
    "        f: a callable that takes input x[:, 2] and returns f(x) of size [:, 1]\n",
    "    \"\"\"\n",
    "    # Get mesh grid\n",
    "    x = np.arange(*x_range, delta).astype('float32')\n",
    "    y = np.arange(*y_range, delta).astype('float32')\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    stacked = np.column_stack([X.ravel(), Y.ravel()])\n",
    "    z_pred = f(stacked)\n",
    "    Z = z_pred.reshape(X.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.contourf(X, Y, Z, **contour_kwargs)\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.xticks(np.linspace(*x_range, 5))\n",
    "    plt.yticks(np.linspace(*y_range, 5))\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_nn = Sequential()\n",
    "shallow_nn.add(Dense(150, activation='relu', bias_initializer='glorot_normal'))\n",
    "shallow_nn.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(shallow_nn.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a neat summary of our model using the `summary` method. \n",
    "\n",
    "This gives a summary of our layers as well as the number of parameters.\n",
    "\n",
    "\n",
    "**Remark.**\n",
    "Note that for models created from `Sequential`, because it doesn't know the input shape, you will not be able to print a summary before calling it at least once (either train or inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us instead build a deep neural network consisting of 3 hidden layers, but each layer is narrower than before. Our goal is to see how the landscape may differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn = Sequential()\n",
    "for _ in range(3):\n",
    "    deep_nn.add(Dense(units=16, activation='relu', bias_initializer='glorot_normal'))\n",
    "deep_nn.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(deep_nn.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that a deeper network with a similar number of parameters (i.e. narrow layers) seems to have a more \"rugged\" landscape. This is a commonly observed, and is what makes deep networks sometimes hard to train.\n",
    "\n",
    "We can go even deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn = Sequential()\n",
    "for _ in range(6):\n",
    "    deep_nn.add(Dense(units=10, activation='relu', bias_initializer='glorot_normal'))\n",
    "deep_nn.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(deep_nn.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Residual Network\n",
    "\n",
    "One of the key recent innovation which made deep models competitive is the invention of the residual network [1]. Here, instead of the connection\n",
    "$$\n",
    "    h^{(i+1)} = f^{(i)}(h^{(i)}, \\theta^{(i)})\n",
    "$$\n",
    "we replace them by\n",
    "$$\n",
    "    h^{(i+1)} = h^{(i)} + f^{(i)}(h^{(i)}, \\theta^{(i)})\n",
    "$$\n",
    "Note: this has important connection to differential equations and numerical analysis, e.g. see [2], [3].\n",
    "\n",
    "We will test this variant of deep networks. We will also take this opportunity to introduce an alternative model API to `Sequential`. This is known as the [functional API](https://keras.io/getting-started/functional-api-guide/) and is one of the most flexible way to build models. The reason why `Sequential()` is not easily applied here is that the $i+1$ layer is connected to both the activation of the previous layer, as well as the outputs from the previous $f^{(i)}$.\n",
    "\n",
    "\n",
    "**Remark.**\n",
    "The most flexible way is actually model-subclassing, but it does sacrifice certain useful features, such as serialization. For more information, see [documentation](https://keras.io/models/about-keras-models/).\n",
    "\n",
    "Note that for models created from `Sequential`, because it doesn't know the input shape, you will not be able to print a summary before calling it at least once (either train or inference).\n",
    "\n",
    "References:\n",
    "\n",
    "    [1] He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. CoRR 2015, abs/1512.0. https://doi.org/10.1109/CVPR.2016.90.\n",
    "    [2] Li, Q.; Chen, L.; Tai, C.; E, W. Maximum Principle Based Algorithms for Deep Learning. The Journal of Machine Learning Research 2017, 18 (1), 5998–6026.\n",
    "    [3] E, W.; Han, J.; Li, Q. A Mean-Field Optimal Control Formulation of Deep Learning. Research in the Mathematical Sciences 2019, 6 (1), 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(2, ))  # For the functional API we will need the input layer\n",
    "dense_layer = Dense(units=10, activation='tanh', bias_initializer='glorot_normal')\n",
    "h = dense_layer(x)\n",
    "for _ in range(5):\n",
    "    dense_layer = Dense(units=10, activation='relu', bias_initializer='glorot_normal')\n",
    "    add_layer = Add()\n",
    "    f = dense_layer(h)\n",
    "    h = add_layer([h, f])\n",
    "output_layer = Dense(units=1)\n",
    "y_hat = output_layer(h)\n",
    "\n",
    "residual_nn = Model(inputs=x, outputs=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(residual_nn.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train each of these networks on some dataset of your choice.\n",
    "  * Which has the best expressive power?\n",
    "  * Which is the easiest to train?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
