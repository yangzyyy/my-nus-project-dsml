{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "Here, we will apply some regularization techniques to improve the performance of a classifier on the CIFAR10 dataset. This is a classic dataset for benchmarking deep learning applications, and strikes a nice balance between task difficulty and computational complexity -- it is harder than MNIST/FashionMNIST, and yet much smaller than the ImageNet dataset. Hence, it is widely used for benchmarking.\n",
    "\n",
    "Here, we will learn how to regularize the models to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/944/1*6XQqOifwnmplS22zCRRVaw.png \"CIFAR10\")\n",
    "\n",
    "More information: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these are RGB images, each sample is going to be of shape [Height, Width, Channels], where the last dimension represents the R, G and B channel of the image. We can still use `plt.imshow` to plot the RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "with sns.axes_style('white'):\n",
    "    plt.imshow(x_train[0])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple CNN with 3 Conv layers and 3 FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv 1\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "# FC 1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# FC 2\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# FC 3\n",
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first fit the model using a \"default\" way, without any regularizations, and see how is the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),  # from_logits=True requires no softmax activation\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history = pd.read_json('cifar10_baseline.json')\n",
    "    model = tf.keras.models.load_model('cifar10_baseline.h5')\n",
    "except:    \n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=0,\n",
    "        callbacks=[TqdmCallback(verbose=1)],\n",
    "    )\n",
    "    history = pd.DataFrame(history.history) \n",
    "    history.to_json('cifar10_baseline.json')\n",
    "    model.save('cifar10_baseline.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "history.plot(y=['loss', 'val_loss'], ax=ax[0])\n",
    "history.plot(y=['accuracy', 'val_accuracy'], ax=ax[1])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Baseline Model\n",
    "\n",
    "Let us now improve the model using the techniques we just covered in class. We will employ\n",
    "  * Dropout\n",
    "  * Batch Normalization\n",
    "  * Momentum SGD with a learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 2\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 3\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# FC 1\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# FC 2\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# FC 3\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SGD (with momentum) optimizer using a learning rate schedule. This can be easily done using the `LearningRateScheduler` callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    \"\"\"\n",
    "    Returns the desired learning rate at epoch\n",
    "    \"\"\"\n",
    "    init_lr = 0.01  # initial\n",
    "    gamma = 0.5  # decay ratio\n",
    "    k0 = 20  # decay interval\n",
    "    lr = init_lr * gamma ** np.floor((epoch)/k0)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=SGD(momentum=0.95),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history = pd.read_json('cifar10_tuned.json')\n",
    "    model = tf.keras.models.load_model('cifar10_tuned.h5')\n",
    "except:\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            TqdmCallback(verbose=1),\n",
    "            LearningRateScheduler(step_decay, verbose=1)\n",
    "        ],\n",
    "    )\n",
    "    history = pd.DataFrame(history.history)\n",
    "    history.to_json('cifar10_tuned.json')\n",
    "    model.save('cifar10_tuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "history.plot(y=['loss', 'val_loss'], ax=ax[0])\n",
    "history.plot(y=['accuracy', 'val_accuracy'], ax=ax[1])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that this has significantly improved performance. In particular, the training/testing performance are now much more similar, as one would expect when successfully regularizing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "As a last method, we will use **data augmentation**. The reason we separate this is to show how drastic an effect this has on the model performance. This is almost *universal*, in the sense that no matter what algorithms/architectures we use, with data augmentation we are likely to see a huge increase in testing performance. This is because we built *prior knowledge* into the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Data Augmentation on Baseline\n",
    "\n",
    "Here, we will only use data augmentation and see what benefit that brings to our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv 1\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "# FC 1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# FC 2\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# FC 3\n",
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform data augmentation on-the-fly during training using the `ImageDataGenerator` class. You can find many other options in it by reading its [documentation](https://keras.io/preprocessing/image/).\n",
    "\n",
    "For this problem we will use\n",
    "  * Random rotation up to 15 degrees\n",
    "  * random shift vertically and horizontally by 10%\n",
    "  * random zoom in and out by 10%\n",
    "  * random horizontal flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    zoom_range=0.1,  # set range for random zoom (1-zoom_range, 1+zoom_range)\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    ")\n",
    "flow = datagen.flow(x_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to call `fit` on the training set\n",
    "```python\n",
    "    datagen.fit(x_train)\n",
    "```\n",
    "if some zca_transform or similar whitening approaches are used. Here we don't have to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history = pd.read_json('cifar10_baseline_augment.json')\n",
    "    model = tf.keras.models.load_model('cifar10_baseline_augment.h5')\n",
    "except:\n",
    "    history = model.fit(\n",
    "        flow,  # Instead of feeding numpy arrays, we feed a generator (in this case, batchsize is not needed)\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=0,\n",
    "        callbacks=[TqdmCallback(verbose=1)],\n",
    "        workers=4,  # Number of parallel workers for the datagen step (should tune this depending on machine)\n",
    "    )\n",
    "    history = pd.DataFrame(history.history)\n",
    "    history.to_json('cifar10_baseline_augment.json')\n",
    "    model.save('cifar10_baseline_augment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "history.plot(y=['loss', 'val_loss'], ax=ax[0])\n",
    "history.plot(y=['accuracy', 'val_accuracy'], ax=ax[1])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that with data augmentation alone, we are able to improve performance drastically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on Regularized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will combine the regularization techniques with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 2\n",
    "# model.add(Dropout(rate=0.25))  # We remove dropouts in conv layers to reduce underfitting\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv 3\n",
    "# model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# FC 1\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# FC 2\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# FC 3\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    ")\n",
    "flow = datagen.flow(x_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=SGD(momentum=0.95),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history = pd.read_json('cifar10_tuned_augment.json')\n",
    "    model = tf.keras.models.load_model('cifar10_tuned_augment.h5')\n",
    "except:\n",
    "    history = model.fit(\n",
    "        flow,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            TqdmCallback(verbose=1),\n",
    "            LearningRateScheduler(step_decay)\n",
    "        ],\n",
    "    )\n",
    "    history = pd.DataFrame(history.history)\n",
    "    history.to_json('cifar10_tuned_augment.json')\n",
    "    model.save('cifar10_tuned_augment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "history.plot(y=['loss', 'val_loss'], ax=ax[0])\n",
    "history.plot(y=['accuracy', 'val_accuracy'], ax=ax[1])\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
