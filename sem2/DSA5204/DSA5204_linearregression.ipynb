{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we introduce basic linear models for regression and classification. We will use basic functionalities of the machine learning library `scikit-learn`. Install this by\n",
    "```\n",
    "$pip install scikit-learn\n",
    "```\n",
    "Documentation is found [here](https://scikit-learn.org/stable/).\n",
    "\n",
    "We will also need `pandas`, `numpy` and plotting libraries `matplotlib` and `seaborn`.\n",
    "```\n",
    "$pip install numpy pandas matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(123)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singapore Housing Dataset\n",
    "\n",
    "This dataset is obtained from a [Govtech database](https://data.gov.sg). Read the description in the website for more information. We are going to only use a subset of this data as a simple demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The data has been downloaded into the repository for convenience. You also also get this (and more) [here](https://data.gov.sg/dataset/resale-flat-prices). We will only use the dataset `resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv` in this simple demo. Besure to place the data in a directory `data/` relative to the directory of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv('./data/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Some data should be numerical but we can't easily work with them, so let's do some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_years(years_and_months):\n",
    "    \"\"\"\n",
    "    Convert n years m months to (n + m/12) years\n",
    "    \n",
    "    \"\"\"\n",
    "    split = years_and_months.split(' ')\n",
    "    if len(split) == 2:\n",
    "        return float(split[0])\n",
    "    elif len(split) == 4:\n",
    "        return float(split[0]) + float(split[2]) / 12.0\n",
    "    else:\n",
    "        raise ValueError('Wrong format.')\n",
    "\n",
    "def average_storey(storey_range):\n",
    "    \"\"\"\n",
    "    Convert n TO m to (n+m)/2\n",
    "    \"\"\"\n",
    "    split = storey_range.split(' TO ')\n",
    "    if len(split) == 2:\n",
    "        return 0.5 * (float(split[0]) + float(split[1]))\n",
    "    else:\n",
    "        raise ValueError('Wrong format.')\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset['remaining_lease'] = dataset['remaining_lease'].apply(convert_to_years)\n",
    "dataset['storey'] = dataset['storey_range'].apply(average_storey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Visualizations\n",
    "\n",
    "We will visualize the data to find some patterns. The `seaborn` plotting library has some useful tools to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 8))\n",
    "\n",
    "sns.barplot(\n",
    "    x='resale_price',\n",
    "    y='town',\n",
    "    data=dataset,\n",
    "    orient='h',\n",
    "    ci=\"sd\",\n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    x='resale_price',\n",
    "    y='storey',\n",
    "    data=dataset,\n",
    "    orient='h',\n",
    "    ci=\"sd\",\n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x='resale_price',\n",
    "    y='floor_area_sqm',\n",
    "    hue='remaining_lease',\n",
    "    data=dataset,\n",
    "    ax=ax[2]\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xticklabels(a.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "We will know proceed to fit some models to predict the *resale_price* given some input descriptors. This is a classical regression problem. \n",
    "\n",
    "To evaluate our machine learning models, we should have at least 1 hold-out test set that is untouched by our learning algorithm, until evaluation time. Since no hyper-parameter tuning is performed, it is sufficient to keep a single test set for this purpose. We use some handy functions from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "We can see from the visualization that floor area is related to price. Rather, we should know this without seeing any data. We now try to regress the resale price from floor area alone and see how we do. \n",
    "\n",
    "Instead of coding our own ordinary least squares solver, `sklearn` (and many other libraries) have ready-made implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'floor_area_sqm'\n",
    "outputs = 'resale_price'\n",
    "\n",
    "x_train = dataset_train[inputs][:, np.newaxis]\n",
    "x_test = dataset_test[inputs][:, np.newaxis]\n",
    "\n",
    "y_train = dataset_train[outputs]\n",
    "y_test = dataset_test[outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    ")\n",
    "y_hat_train = regressor.predict(x_train)\n",
    "y_hat_test = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Regression Result\n",
    "\n",
    "Since we are working in one dimension, it is possible to visualize our linear fit.\n",
    "\n",
    "Note that the following can be directly reproduced by a single `sns.regplot` call from `seaborn`, without explicitly using `LinearRegression()` from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=x_train.ravel(),\n",
    "    y=y_train,\n",
    "    ax=ax[0],\n",
    "    alpha=0.5,\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=x_train.ravel(),\n",
    "    y=y_hat_train,\n",
    "    ax=ax[0],\n",
    "    color='red',\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=x_test.ravel(),\n",
    "    y=y_test,\n",
    "    ax=ax[1],\n",
    "    alpha=0.5,\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=x_test.ravel(),\n",
    "    y=y_hat_test,\n",
    "    ax=ax[1],\n",
    "    color='red',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(inputs)\n",
    "    a.set_ylabel(outputs)\n",
    "\n",
    "    \n",
    "ax[0].set_title('Train set')\n",
    "ax[1].set_title('Test set')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can directly compare our predicted resale prices with the ground truth. This works if we have higher dimensional inputs so that a linear fit is difficult to plot graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=y_train,\n",
    "    y=y_hat_train,\n",
    "    ax=ax[0],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=y_test,\n",
    "    y=y_hat_test,\n",
    "    ax=ax[1],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel('{} (true)'.format(outputs))\n",
    "    a.set_ylabel('{} (predict)'.format(outputs))\n",
    "    a.set_xlim(1.5*10**5, 10**6)\n",
    "    a.set_ylim(1.5*10**5, 10**6)\n",
    "    a.plot(a.get_xlim(), a.get_ylim(), ls='--', c='k')\n",
    "\n",
    "\n",
    "    \n",
    "ax[0].set_title('Train set')\n",
    "ax[1].set_title('Test set')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying the Error\n",
    "\n",
    "Just looking at the fit doesn't tell us very precise information. Therefore, it is often better to quantify the error. One simple metric is the mean-square difference between the predicted outputs and the actual outputs. But this is not the only one! You are encouraged to look at [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) to find more ways to evaluate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def rmse_scaled(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    scale = np.sqrt(np.mean(y_true**2))\n",
    "    return rmse/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Scaled RMSE: \\n {} (Train) \\n {} (Test)'.format(\n",
    "        rmse_scaled(y_train, y_hat_train),\n",
    "        rmse_scaled(y_test, y_hat_test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Simple linear regression gave about 26% error. Can we do better by going to larger hypothesis spaces? Let us now try a **polynomial regression** up to degree 3. This is an example of a linear basis model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = PolynomialFeatures(degree=3)\n",
    "x_poly_train = phi.fit_transform(x_train)\n",
    "x_poly_test = phi.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_poly = LinearRegression()\n",
    "regressor_poly.fit(\n",
    "    X=x_poly_train,\n",
    "    y=y_train,\n",
    ")\n",
    "y_hat_poly_train = regressor_poly.predict(x_poly_train)\n",
    "y_hat_poly_test = regressor_poly.predict(x_poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=x_train.ravel(),\n",
    "    y=y_train,\n",
    "    ax=ax[0],\n",
    "    alpha=0.5,\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=x_train.ravel(),\n",
    "    y=y_hat_poly_train,\n",
    "    ax=ax[0],\n",
    "    color='red',\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=x_test.ravel(),\n",
    "    y=y_test,\n",
    "    ax=ax[1],\n",
    "    alpha=0.5,\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=x_test.ravel(),\n",
    "    y=y_hat_poly_test,\n",
    "    ax=ax[1],\n",
    "    color='red',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(inputs)\n",
    "    a.set_ylabel(outputs)\n",
    "\n",
    "    \n",
    "ax[0].set_title('Train set')\n",
    "ax[1].set_title('Test set')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=y_train,\n",
    "    y=y_hat_poly_train,\n",
    "    ax=ax[0],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=y_test,\n",
    "    y=y_hat_poly_test,\n",
    "    ax=ax[1],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel('{} (true)'.format(outputs))\n",
    "    a.set_ylabel('{} (predict)'.format(outputs))\n",
    "    a.set_xlim(1.5*10**5, 10**6)\n",
    "    a.set_ylim(1.5*10**5, 10**6)\n",
    "    a.plot(a.get_xlim(), a.get_ylim(), ls='--', c='k')\n",
    "\n",
    "\n",
    "    \n",
    "ax[0].set_title('Train set')\n",
    "ax[1].set_title('Test set')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Scaled RMSE: \\n {} (Train) \\n {} (Test)'.format(\n",
    "        rmse_scaled(y_train, y_hat_poly_train),\n",
    "        rmse_scaled(y_test, y_hat_poly_test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like polynomial regression didn't really improve on the simple linear regression result. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including More Design Variables in Linear Regression\n",
    "\n",
    "Clearly, the size of the house is not the only factor determining price. Let us now include some other factors which may contribute. In this case, the regression problem is defined in higher dimenions.\n",
    "\n",
    "Note that from the viewpoint of approximation, we are also using a bigger **hypothesis space**, but not in the same way as a polynomial basis in one dimension. In the first linear regression example, we are using the hypothesis space consisting of all functions which are affine in *floor_area_sqm* and constant in the other dependent variables. Now, we are using the hypothesis space consisting of affine functions of more than one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['town', 'floor_area_sqm', 'remaining_lease', 'storey']\n",
    "outputs = 'resale_price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset_train[dataset_train.columns.intersection(inputs)]\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"town\" column are nominal/categorical variables, so let us perform a **one-hot encoding**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.get_dummies(x_train)\n",
    "y_train = dataset_train[outputs]\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we repeat this for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = dataset_test[dataset_test.columns.intersection(inputs)]\n",
    "x_test = pd.get_dummies(x_test)\n",
    "y_test = dataset_test[outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we train the regressor in these new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    ")\n",
    "y_hat_train = regressor.predict(x_train)\n",
    "y_hat_test = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=y_train,\n",
    "    y=y_hat_train,\n",
    "    ax=ax[0],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=y_test,\n",
    "    y=y_hat_test,\n",
    "    ax=ax[1],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel('{} (true)'.format(outputs))\n",
    "    a.set_ylabel('{} (predict)'.format(outputs))\n",
    "    a.set_xlim(1.5*10**5, 10**6)\n",
    "    a.set_ylim(1.5*10**5, 10**6)\n",
    "    a.plot(a.get_xlim(), a.get_ylim(), ls='--', c='k')\n",
    "\n",
    "\n",
    "    \n",
    "ax[0].set_title('Train set')\n",
    "ax[1].set_title('Test set')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Scaled RMSE: \\n {} (Train) \\n {} (Test)'.format(\n",
    "        rmse_scaled(y_train, y_hat_train),\n",
    "        rmse_scaled(y_test, y_hat_test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe from the fit and the RMSE that we are doing quite a bit better than the univariate linear case. Can you improve on this:\n",
    "  *  Better accuracy?\n",
    "  *  Smaller number of features?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
