{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we apply RNN and its variants to predict the positivity of movie reviews. This notebook serves the following purposes:\n",
    "  * Introduce basic usage of RNN/LSTM\n",
    "  * Introduce basic text data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/) consisting of movie reviews of various movie titles. \n",
    "\n",
    "Our goal is to develop a machine learning model which can predict, given a text review, whether the sentiment of the review is postive (1) or negative (0). \n",
    "\n",
    "THe IMDB dataset is built into `keras.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the inputs are already coded into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actually word encodings based on frequency. For details, have a look a the documentation of the dataset, e.g. [here](http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjE69P55NXnAhXLfH0KHSFoClAQygQwAHoECAQQBw&url=http%3A%2F%2Fkeras.io%2Fdatasets%2F%23imdb-movie-reviews-sentiment-classification&usg=AOvVaw3ZEeYraF9cI7oBodf2K9ea).\n",
    "\n",
    "To see what the review text actually is, we write a simple function that converts the encodings back into words. This is done using the `imdb.get_word_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(word_ids):\n",
    "    \"\"\"\n",
    "    Convert list of word_ids back to words.\n",
    "    Special chars for 0-3 are based on the default kwargs of\n",
    "    imdb.load_data()\n",
    "    \"\"\"\n",
    "    index_from = 3\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k: (v + index_from) for k, v in word_to_id.items()}\n",
    "    word_to_id['<PAD>'] = 0\n",
    "    word_to_id['<START>'] = 1\n",
    "    word_to_id['<UNK>'] = 2\n",
    "    word_to_id['<UNUSED>'] = 3\n",
    "    id_to_word = {value: key for key, value in word_to_id.items()}\n",
    "    return ' '.join(id_to_word[id] for id in word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some randomly chosen reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(x_train))\n",
    "print('Sentiment: ', y_train[idx])\n",
    "print('Review: ', to_words(x_train[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what is the length (number of words) for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_train = list(map(len, x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(lengths_train, kde=True)\n",
    "ax.set_xlabel('Number of Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words clearly varies from review to review, so we pad them. This is performed by the `sequence.pad_sequences` function. Any review longer than `maxlen` is cut off, and those shorter are padded (in the front) by 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Simple RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to build a RNN model to learn to predict the sentiment given the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we cannot directly work with x_train because they are encoded integers. Instead, we first use an `Embedding` layer that casts these integer encodings to a feature space. \n",
    "\n",
    "For example, `Embedding(5, 2)` is a 5x2 matrix that maps integer encodings (0,...,4) into 5 real-valued vectors of 2 dimensions:\n",
    "\n",
    "| Coding | Embedded Vectors\n",
    "| --- | --- |\n",
    "| 0 | [0.5, 1.0] |\n",
    "| 1 | [1.0, 1.2] |\n",
    "| 2 | [0.1, -0.6] |\n",
    "| 3 | [0.3, 0.5] |\n",
    "| 4 | [-0.4, -0.1] |\n",
    "\n",
    "This embedding is trainable, so we can learn to embed these encodings in the right way relevant to the task: words of similar meaning should have similar embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(20000, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add the RNN layers, and append to it an output `Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to test several models, we will write some functions to train, save and evaluate models. These functions are self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(model, path, force=False, optimizer=Adam(0.001)):\n",
    "    \"\"\"\n",
    "    Looks for saved model in path, if found, load.\n",
    "    If not, compile, train and save model to path\n",
    "    If force=True, will always retrain\n",
    "    \"\"\"\n",
    "    model_save_dir = pathlib.Path(path)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    if model_save_dir.exists() and not force:\n",
    "        model.load_weights(str(model_save_dir))\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=32,\n",
    "            epochs=15,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[TqdmCallback(verbose=1)],\n",
    "            verbose=0,\n",
    "        )\n",
    "        model.save_weights(str(model_save_dir))\n",
    "        results = pd.DataFrame(history.history)\n",
    "        results['epoch'] = history.epoch\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate model on train/test sets\n",
    "    \"\"\"\n",
    "    eval_train = model.evaluate(*train_data, batch_size=512, verbose=0)\n",
    "    eval_test = model.evaluate(*test_data, batch_size=512, verbose=0)\n",
    "    print(f'Train - loss = {eval_train[0]:.3f}, acc = {eval_train[1]:.3f} ')\n",
    "    print(f'Test - loss = {eval_test[0]:.3f}, acc = {eval_test[1]:.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save(model=model, path='imdb_simple_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_data=(x_train, y_train), test_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train a deep RNN model by stacking two RNN cells together. This is done simply by adding another `model.add` call. However, note that other than the last RNN cell, \"hidden\" RNN cells need to have `return_sequences` set to `True`, so that the entire hidden sequence $h^{(t)}$, is returned, and hence can be treated as inputs for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(SimpleRNN(128, return_sequences=True))\n",
    "model.add(SimpleRNN(64, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are notoriously hard to train. For this deeper model, we will use a smaller learning rate than before for the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save(model=model, path='imdb_deep_rnn.h5', optimizer=Adam(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_data=(x_train, y_train), test_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that by going deeper we actually manage to do a little better than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also try to improve performance using LSTM, which makes learning long-time dependence much easier. The implementation is very simple -- we just substitute all calls to `SimpleRNN` by `LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save(model=model, path='imdb_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_data=(x_train, y_train), test_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play around with the above models and optimizer configurations to get better models.\n",
    "2. Observe that the training accuracy is much greater than the test accuracy. What can you do to improve generalization?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
