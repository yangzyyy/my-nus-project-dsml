{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we consider a sequence prediction problem. Our goal is to illustrate a different setting from the IMDB sentiment analysis problem, where the prediction output is no longer just 1 label.\n",
    "\n",
    "Goals:\n",
    "1. `return_sequences` keyword\n",
    "2. exploit properties in task to use right scaling/activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tqdm\n",
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid 19 Dataset\n",
    "\n",
    "Information: https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.api.dataset_download_files(\n",
    "    'sudalairajkumar/novel-corona-virus-2019-dataset',\n",
    "    path='./data/covid19',\n",
    "    quiet=False,\n",
    "    unzip=True,\n",
    "    force=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Minimal Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read in the CSV files and look at its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_confirmed = pd.read_csv('data/covid19/time_series_covid_19_confirmed.csv')\n",
    "data_deaths = pd.read_csv('data/covid19/time_series_covid_19_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_confirmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deaths.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract some numpy arrays of the counts, and country names for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_confirmed = np.asarray(data_confirmed)[:, 4:].astype('float64')\n",
    "number_deaths = np.asarray(data_deaths)[:, 4:].astype('float64')\n",
    "countries = np.asarray(data_confirmed['Country/Region'])\n",
    "provinces = np.asarray(data_confirmed['Province/State'].fillna(''))\n",
    "names = [f'{c} {p}' for c, p in zip(countries, provinces)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers are rather large, so we take a logarithm scaling to control the magnitude. Why is 1.0 added?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_confirmed = np.log(1.0 + number_confirmed)\n",
    "number_deaths = np.log(1.0 + number_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can plot the numbers and see some rough trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    ax[0].plot(number_confirmed[i], label=countries[i])\n",
    "    ax[1].plot(number_deaths[i], label=countries[i])\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.set_xlabel('days')\n",
    "    a.set_ylabel('numbers (log)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will build a model that links the confirmed cases to the number of deaths.\n",
    "\n",
    "We know that there is a link, but there is also a time lag - we cannot just use the same day's confirmed cases to predict that days number of deaths.\n",
    "\n",
    "However, we should expect a link if we look at all the cumulative confirmed counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now keep 20% of the countries data as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, names_train, names_test = train_test_split(\n",
    "    number_confirmed[:, :, None], number_deaths[:, :, None], names,\n",
    "    test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build a simple LSTM model for this, using the canned layers from `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=[None, x_train.shape[-1]]))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss='mse', optimizer=Adam(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = pathlib.Path('covid_lstm.h5')\n",
    "\n",
    "if model_save_dir.exists():\n",
    "    model.load_weights(str(model_save_dir))\n",
    "else:\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        callbacks=[TqdmCallback(verbose=1)]\n",
    "    )\n",
    "    model.save_weights(str(model_save_dir))\n",
    "    results = pd.DataFrame(history.history)\n",
    "    results['epoch'] = history.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us look at the predictions on the test countries/provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.squeeze(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 3\n",
    "n_cols = 5\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        count = i * n_cols + j\n",
    "        ax[i,j].plot(y_test[count], label='True')\n",
    "        ax[i,j].plot(y_pred[count], label='Predicted')\n",
    "        \n",
    "        ax[i,j].legend()\n",
    "        ax[i,j].set_title(names_test[count])\n",
    "        ax[i,j].set_xlabel('days')\n",
    "        ax[i,j].set_ylabel('numbers (log)')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to modify the target to be a 10-day advance prediction, i.e. the task is to predict the number of deaths 10 days from the current, given the current knowledge of confirmed cases.\n",
    "2. Try without log scaling, or without ReLU activation. These are called ablation studies\n",
    "3. Try improving the model in other ways (we will learn some techniques in the following classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
