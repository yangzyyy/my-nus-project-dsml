{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA5101 - Introduction to Big Data for Industry\n",
    "\n",
    "\n",
    "**Prepared by *Dr Li Xiaoli*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Performance Evaluation\n",
    "\n",
    "## Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. \n",
    "\n",
    "## In particular any evaluation metric should not take the absolute values of the cluster labels into account, but rather if this clustering results define separations of the data similar to some ground truth set of classes, or satisfying some assumption, such that *members belong to the same class are more similar than members from different classes*, according to some similarity metric. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ground truth labels are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Adjusted Rand index\n",
    "\n",
    "\n",
    "> Given the knowledge of the ground truth class assignments **labels_true** and\n",
    "our clustering algorithm assignments of the same samples **labels_pred**,\n",
    "\n",
    "> the adjusted Rand index is a function that **measures the similarity of the \n",
    "two assignments**, ignoring permutations and with chance normalization\n",
    "\n",
    "> Basically, even though we already know the predicted results and grouth truth, we still need to define some simiarltiy metrics to measure how good are our predicted results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24242424242424243\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]   \n",
    "# This predicted results (labels_pred) only indicate \n",
    "# first two examples belong to same class, \n",
    "# Middle two examples belong to same class but they do not belong to the same class as the first two examples\n",
    "# Last two examples belong to same class, but they do not belong to the same class as the first two examples and middle two examples\n",
    "\n",
    "score=metrics.adjusted_rand_score(labels_true, labels_pred)  \n",
    "# The Rand Index computes a similarity measure between two clusterings by considering all pairs of \n",
    "# samples and counting pairs that are assigned in the same or different clusters \n",
    "# in the predicted and true clusterings.\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help (metrics.adjusted_rand_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24242424242424243\n"
     ]
    }
   ],
   "source": [
    "# Original predcited labels [0, 0, 1, 1, 2, 2]\n",
    "# Let us conduct permutation twice\n",
    "# 1. One can permute 0 and 1 in the predicted labels, # 1->0, 0->1, \n",
    "# [0, 0, 1, 1, 2, 2]=> [1, 1, 0, 0, 2, 2]\n",
    "\n",
    "# 2. rename 2 to 3, 2->3\n",
    "# [1, 1, 0, 0, 2, 2]=> [1, 1, 0, 0, 3, 3]\n",
    "\n",
    "# The results should NOT change, as we simply do permutation, i.e. \n",
    "# only assigning different clusters with different cluster labels\n",
    "# In the last assignment, labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "# Now we perform permutations and change it to: \n",
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "permute_score=metrics.adjusted_rand_score(labels_true, labels_pred)  \n",
    "print (permute_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24242424242424243\n"
     ]
    }
   ],
   "source": [
    "# Furthermore, adjusted_rand_score is symmetric: swapping the argument, \n",
    "#  i.e. predicted labels and grouth-truth labels\n",
    "# does not change the score. It can thus be used as \n",
    "# a consensus measure: Good property\n",
    "Swap_score=metrics.adjusted_rand_score(labels_pred, labels_true)  \n",
    "print (Swap_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mutual Information based scores\n",
    "\n",
    "\n",
    "> Given the knowledge of the ground truth class assignments **labels_true** \n",
    "and our clustering algorithm assignments of the same samples **labels_pred**,\n",
    "the Mutual Information is a function that measures the **agreement of the\n",
    " two assignments**, ignoring permutations. \n",
    "\n",
    "\n",
    "> Two different normalized versions of this measure are available, \n",
    "Normalized Mutual Information(NMI) and Adjusted Mutual Information(AMI). \n",
    "\n",
    "> NMI is often used in the literature while AMI was proposed more recently \n",
    "and is normalized against chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2987924581708901\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "NMI_score=metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "print (NMI_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  help (metrics.adjusted_mutual_info_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:\n",
    "\n",
    "#### All metrics, including mutual_info_score, adjusted_mutual_info_score and normalized_mutual_info_score, are symmetric: \n",
    "\n",
    "#### swapping the argument does not change the score. Thus they can be used as a consensus measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2987924581708901\n"
     ]
    }
   ],
   "source": [
    "# Now we perform permutations\n",
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "NMI_score=metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "print (NMI_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2987924581708903\n"
     ]
    }
   ],
   "source": [
    "# swapping the argument,\n",
    "NMI_swap_score=metrics.adjusted_mutual_info_score(labels_pred, labels_true)\n",
    "print (NMI_swap_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 Homogeneity, completeness and V-measure\n",
    "\n",
    "\n",
    "### Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis. In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:\n",
    "\n",
    "> homogeneity: each cluster contains only members of a single class. Describe purity\n",
    "\n",
    "> completeness: all members of a given class are assigned to the same cluster. Concern about recall\n",
    "\n",
    "### We can turn these two concepts as two scores homogeneity_score and completeness_score. Both are bounded below by 0.0 and above by 1.0 (higher is better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666669"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.homogeneity_score(labels_true, labels_pred)  \n",
    "\n",
    "# according to their definitions, this example has relatively good homogenity \n",
    "# and relatively bad completeness\n",
    "# homogenity: take a look at the predicted labels [0, 0, 1, 1, 2, 2]\n",
    "# 0, 0: aligns with the ground truth\n",
    "# 1, 1: not correct (correct one is 0, 1)\n",
    "# 2, 2: aligns with the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.420619835714305"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.completeness_score(labels_true, labels_pred) \n",
    "# 0, 0, 0 missing one or 1/3\n",
    "# 1, 1, 1 missing one or 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666669, 0.420619835714305, 0.5158037429793889)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Their harmonic mean called V-measure is computed by v_measure_score:\n",
    "metrics.v_measure_score(labels_true, labels_pred)    \n",
    "\n",
    "#The V-measure is actually equivalent to the mutual information (NMI) \n",
    "# discussed above normalized by the sum of the label entropies.\n",
    "\n",
    "#Homogeneity, completeness and V-measure can be computed at once \n",
    "#using homogeneity_completeness_v_measure (three in one)\n",
    "\n",
    "metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
    "\n",
    "#3 scores: Homogeneity, completeness and V-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.6853314789615865, 0.8132898335036762)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following clustering assignment is slightly better, \n",
    "#since it is homogeneous (take a look at the predicted labels, \n",
    "# consisting with true labels), but not complete:\n",
    "labels_true= [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 0, 1, 2, 2]\n",
    "metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
    "\n",
    "# Take a look at the predicted labels\n",
    "# Homogeneity score is 1, as each cluster contains only members of \n",
    "# a single class and thus pure (first 3, middle 1, last 2, all pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ground truth labels are not known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient\n",
    "\n",
    "#### If the ground truth labels are not known, evaluation must be performed using the model itself. \n",
    "#### The Silhouette Coefficient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters.   The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each exampe. \n",
    "\n",
    "#### More specifically, the Silhouette Coefficient is defined for each sample and is composed of two scores:\n",
    "\n",
    "> a: The mean distance between an example and all other points in the same class.\n",
    "\n",
    "> b: The mean distance between an example and all other points in its nearest cluster.\n",
    "\n",
    "#### The Silhouette Coefficient s for a single example is then given as:\n",
    "> s = $\\frac{b - a}{max(a, b)}$\n",
    "\n",
    "#### Usually b>a, as each example should be near to its own cluster examples, instead of near to its nearest cluster examples. \n",
    "\n",
    "##### The Silhouette Coefficient for a set of examples is given as the mean of the Silhouette Coefficient for each example.\n",
    "\n",
    "#### Bigger s values, indicating we have good cluster structure, which each cluster is well seperated to other clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help (metrics.silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5528190123564091"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# Prepare IRIS data \n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target # NOTE WE DONOT USE y to evaluate \n",
    "\n",
    "\n",
    "#In normal usage, the Silhouette Coefficient is applied to the results \n",
    "# of a cluster analysis and thus has not groud truth labels\n",
    "\n",
    "# Perform KMeans Clustering\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_ # Clustering results\n",
    "\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "# X is the data (without label)\n",
    "# labels is the clustering results, i.e. predicted labels\n",
    "# We use euclidean for distance calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relative good clustering results, as silhouette_score is big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels # clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # Ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7514854021988339, 0.7649861514489816, 0.7581756800057786)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.homogeneity_completeness_v_measure(y, labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
