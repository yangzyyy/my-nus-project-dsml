{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA5101 - Introduction to Big Data for Industry\n",
    "\n",
    "\n",
    "**Prepared by *Dr Li Xiaoli*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of machine learning models) assume that all features are centered around zero and have variance in the same order. \n",
    "\n",
    "## If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "## The function **scale** provides a quick and easy way to perform this operation on a single array-like dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sklearn provides data preprocessing capabilities, we will learn a few important ones\n",
    "\n",
    "> We will also use numpy to generate array data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Scaled  features - zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input packages preprocessing\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  2.],\n",
       "       [ 2.,  0.,  0.],\n",
       "       [ 0.,  1., -1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate array data\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "               [ 2.,  0.,  0.],\n",
    "               [ 0.,  1., -1.]])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform scaling on data using preprocessing.scale (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Scaling on Columns/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features = preprocessing.scale(X) # default scaling on columns/features, i.e. axis=0\n",
    "X_scaled_features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below we get the same results, i.e. with or without axis=0 is the same\n",
    "X_scaled_features = preprocessing.scale(X, axis=0) # default scaling on columns/features, i.e. axis=0\n",
    "X_scaled_features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled data has zero mean and unit variance, in terms of 3 columns or features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1.1 Let us check/verify X_scaled_features' mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features.mean(axis=0)\n",
    "# axis : int (0 by default)\n",
    "# axis used to compute the means and standard deviations along given along axis. \n",
    "# If axis=0, independently standardize each feature (vertical), \n",
    "# otherwise (if axis=1) standardize each sample (horizontal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us take a look at the mean of third column\n",
    "(1.33630621-0.26726124-1.06904497)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute variance of first column according to the variance formula, given average is 0\n",
    "#### $\\frac { \\sum (every value-mean)^2} {n}$= $\\frac { \\sum (every value-0)^2} {3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit variance 1\n"
     ]
    }
   ],
   "source": [
    "firstcolumnvariance=(((0-0)**2+(1.22474487-0)**2+(-1.22474487-0)**2)/3)\n",
    "print (\"unit variance %.0f\" % firstcolumnvariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This means all the columns have zero mean\n",
    "#### We can also see it directly\n",
    "#### The key advantage is that we do not want some features dominate the future computation, e.g. similarity, distance, objective function, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, what about means for rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1.2 Let us check/verify X_scaled_features' rows' mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03718711,  0.31916121, -0.35634832])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of first row is 0.03718711\n"
     ]
    }
   ],
   "source": [
    "# manully compute the average of the first row [ 0. , -1.22474487,  1.33630621]\n",
    "firstrowave=(0-1.22474487+1.33630621)/3\n",
    "print(\"The average of first row is %.8f\" % firstrowave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is not zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(X_scaled.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04587533, 0.64957343, 1.11980724])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_features.std(axis=1)\n",
    "# Standard Deviation of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also not a unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let us scale on rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Scaling on Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  2.],\n",
       "       [ 2.,  0.,  0.],\n",
       "       [ 0.,  1., -1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the same data\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "               [ 2.,  0.,  0.],\n",
    "               [ 0.,  1., -1.]])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26726124, -1.33630621,  1.06904497],\n",
       "       [ 1.41421356, -0.70710678, -0.70710678],\n",
       "       [ 0.        ,  1.22474487, -1.22474487]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_rows = preprocessing.scale(X, axis=1)  #axis=1, indicating we are handling/scaling rows\n",
    "X_scaled_rows   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.48029737e-16, 7.40148683e-17, 0.00000000e+00])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_rows.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_rows.std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has zero mean and unit variance for rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get more information about scale from \n",
    "# ? preprocessing.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Scaling features to a range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. \n",
    "\n",
    "### This can be achieved using preprocessing.MinMaxScaler or MaxAbsScaler, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following three features have range [0, 2], [-1, 1] and [-1,2] respectively\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                     [ 2.,  0.,  0.],\n",
    "                     [ 0.,  1., -1.]])\n",
    "\n",
    "# perform MinMax scaler to make them into the same range [0,1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train) # fit training data and transform training data\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The same instance of the transformer can then be applied to some new _unseen test  data_  during the fit call: the same scaling operations will be applied to be consistent with the transformation performed on the train data!\n",
    "#### We learn from training data and then apply to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Suppose we have a test example [ -3., -1.,  4.], which is different from training examples\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "\n",
    "# min_max_scaler is obtained from training data\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the range of test feature values may not be [0,1]  \n",
    "#### as we are obtaining min-max scaling (parameters, i.e. original minimal and maximal values for each feature) based on training data, \n",
    "#### which can only make each feature  in training data within range [0, 1] instead of test data\n",
    "\n",
    "#### Test data features could have bigger or smaller values than the parameters obtained from training data. For example, for feature 1, the two parameters are 0 (minimal) and 2 (maximal) [0, 2] in training data, but now in test data, the first feature value is -3, much smaller than minimal value 0 in training data. So in the end, it has been scaled to -1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Normalization is the process of _scaling individual samples to have unit norm_. \n",
    "### This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the L1 or L2 norms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "      [ 2.,  0.,  0.],\n",
    "      [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2') \n",
    "\n",
    "X_normalized \n",
    "# Here the sum of square of all the elements in normalized vector is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, given [$x_1, x_2, ..., x_n$], its L2 normalization makes sure\n",
    "it normalized vector has $\\sqrt{x_1^2+ x_2^2+...+ x_n^2}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit variance 1\n"
     ]
    }
   ],
   "source": [
    "from math import *   # want to use sqrt function\n",
    "normalized_length = sqrt(0.40824829**2 + (-0.40824829)**2 + 0.81649658**2)\n",
    "print (\"unit variance %.0f\" % normalized_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between scaling and normalization here?\n",
    "#### Scaling can perform on columns (features) or rows (records), e.g. scale to zero mean and unit variance\n",
    "#### Normalization is performed for rows only, to make sure each row (sample/vector) has unit length (say for L-2 norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Imputation of missing values\n",
    "> Many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). \n",
    "\n",
    "> A better strategy is to impute the missing values, i.e., to infer them from the known parts of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> The **Imputer class** provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. \n",
    "\n",
    "> This class also allows for *different* missing values encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> The following snippet demonstrates how to replace missing values, encoded as np.nan, using the **mean value** of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Univariate feature imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Using mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# axis=0, meaning that we want to do it for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple Data: 3 records with 2 dimension\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])  #Fit training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The $\\underline{first}$ feature, using (1+7)/2=4\n",
    "#### The $\\underline{second}$ feature, using (2+3+6)/3=3.66666667\n",
    "#### We need to perform fit to compute mean score for each feature with 'NaN' values\n",
    "#### From training data, we will compute the mean value for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Now, given a new test example, how to handle/transform its missing value?\n",
    "# We use the mean value computed from training data to replace missing values in test/future data\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 using most frequent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [np.nan, \"y\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "# Data Frame is more like database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0    a    x\n",
       "1  NaN    y\n",
       "2    a  NaN\n",
       "3    b    y"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy=\"most_frequent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\underline{First}$ column, NaN has been changed to the most frequent value $a$\n",
    "#### $\\underline{Second}$ column, NaN has been changed to the most frequent value $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'x']\n",
      " ['a' 'y']\n",
      " ['a' 'y']\n",
      " ['b' 'y']]\n"
     ]
    }
   ],
   "source": [
    "print(imp.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multivariate feature imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> IterativeImputer models each feature with missing values as a function of other features, and uses that estimate for imputation. \n",
    "\n",
    "> It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. \n",
    "\n",
    "> A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. \n",
    "\n",
    "> This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterativeImputer(random_state=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]]) # the second feature is double of the first\n",
    "IterativeImputer(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Nearest neighbors imputation¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. \n",
    "\n",
    "> By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. \n",
    "\n",
    "> Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "nan = np.nan\n",
    "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\") \n",
    "#using the mean feature value of the two nearest neighbors of samples with missing values:\n",
    "imputer.fit_transform(X)\n",
    "# Given [1, 2, nan] (third column missing), we find 2 nearest neighbors: [3, 4, 3], [nan, 6, 5]  \n",
    "#(second and third example), (3+5)/2=4, i.e.average the thrid column\n",
    "# Given [nan, 6, 5] (first column missing), we find 2 nearest neighbors: [3, 4, 3],[8, 8, 7]]\n",
    "# (second and fourth example),  (3+8)/2=5.5, i.e.average the first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Generating polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Often it might be useful to add complexity to the model by considering _nonlinear features_ of the input data. \n",
    "\n",
    "### A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. \n",
    "\n",
    "### It is implemented in PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate 3*2 matrix\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "X                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "# degree : integer\n",
    "# The degree of the polynomial features. Default = 2.\n",
    "\n",
    "poly.fit_transform(X)    \n",
    "# The features of X have been transformed from  (X1, X2) to (1, X1, X2, X1*X1, X1*X2, X2*X2.\n",
    "# First 1 is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### In some cases, only interaction terms among features are required, and it can be gotten with the setting interaction_only=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9).reshape(3, 3)\n",
    "X                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\n",
       "       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\n",
       "       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "poly.fit_transform(X)  \n",
    "#The features of X have been transformed from  (X1, X2, X3) \n",
    "#to (1, X1, X2, X3, X1*X2, X1*X3, X2*X3, X1*X2*X3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. \n",
    "### You can implement a transformer from an arbitrary function with FunctionTransformer\n",
    "### For example, to build a transformer that applies a log transformation\n",
    "### It is easy for us to change different transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "# log1p computes log(1 + x)     # log with base e\n",
    "# Return the natural logarithm of one plus the input array, element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 1], [2, 3]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718],\n",
       "       [1.09861229, 1.38629436]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453 1.0986122886681098 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print (math.log(1+1), math.log(2+1), math.log(3+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Often features are not given as continuous values but categorical.\n",
    "\n",
    "* For example a person could have features: \n",
    "\n",
    "> feature 1: [\"female\", \"male\"], \n",
    "\n",
    "> Feature 2: [\"from Asia\", \"from Europe\", \"from US\"], \n",
    "\n",
    "> feature 3: [ \"uses Chrome\", \"uses Firefox\", \"uses Internet Explorer\", uses Safari\"].\n",
    "\n",
    "* Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [1, 2, 2], while [\"female\", \"from Asia\", \"uses Chrome\"] would be [0, 0, 0].\n",
    "* Indices start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['male', 'from US', 'uses Safari'],\n",
       " ['female', 'from Europe', 'uses Firefox'],\n",
       " ['female', 'from Asia', 'uses Chrome'],\n",
       " ['male', 'from US', 'uses Internet Explorer']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = preprocessing.OrdinalEncoder() #enc denote encoder variable\n",
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox'], \n",
    "     ['female', 'from Asia', 'uses Chrome'],\n",
    "     ['male', 'from US', 'uses Internet Explorer']]\n",
    "X\n",
    "# X has 4 examples/records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us sort each feature alphabetically\n",
    "\n",
    "### [\"female\", \"male\"], [\"from Asia\", \"from Europe\", \"from US\"], \n",
    "### [ \"uses Chrome\", \"uses Firefox\", \"uses Internet Explorer\", uses Safari\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 3.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(X)\n",
    "OrdinalEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform([['female', 'from Europe', 'uses Internet Explorer']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "OneHotEncoder()\n",
    "enc.transform([['female', 'from US', 'uses Safari'],\n",
    "               ['male', 'from Europe', 'uses Firefox']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vocabulary _alphabetically_ for each feature, leading to a 6 dimension vector\n",
    "\n",
    "['female', 'male', 'from Europe', 'from US', 'uses Firefox', 'uses Safari']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to specify this explicitly using the parameter categories. There are two genders, four possible continents and four Web browsers in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = ['female', 'male']\n",
    "locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n",
    "browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n",
    "enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "OneHotEncoder(categories=[['female', 'male'],\n",
    "                          ['from Africa', 'from Asia', 'from Europe','from US'],\n",
    "                          ['uses Chrome', 'uses Firefox', 'uses IE','uses Safari']])\n",
    "enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|F1|F2|F3|F4|F5|F6|F7|F8|F9|F10|\n",
    "|------|------|------|------|------|------|------|------|------|------|\n",
    "|'female'| 'male' | 'from Africa'| 'from Asia'| 'from Europe'| 'from US'|'uses Chrome'| 'uses Firefox'| 'uses IE'|'uses Safari'|\n",
    "|1|0|0|1|0|0|1|0|0|0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature value is a feature with binary value: 1 occur; 0 otherwise. \n",
    "There are three 1 (F1, F4, F7) in the onehot coded vector because of 'female', 'from Asia', 'uses Chrome' respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. \n",
    "\n",
    "* Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes.\n",
    "\n",
    "* One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  5., 15.],\n",
       "       [ 0.,  6., 14.],\n",
       "       [ 6.,  3., 11.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "X = np.array([[ -3., 5., 15 ],\n",
    "               [  0., 6., 14 ],\n",
    "               [  6., 3., 11 ]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have three features\n",
    "* Feature 1 has value -3, 0, 6\n",
    "* Feature 2 has value 3, 5, 6\n",
    "* Feature 3 has value 11, 14, 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parttion each feature into multiple bins, e.g. \n",
    "* Feature 1 into _3_ bins\n",
    "* Feature 2 into _2_ bins\n",
    "* Feature 3 into _2_ bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KBinsDiscretizer(encode='ordinal', n_bins=[3, 2, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By default the output is one-hot encoded into a sparse matrix and this can be configured with the encode parameter. For each feature, the bin edges are computed during fit and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as:\n",
    "\n",
    "feature 1: [-$\\infty$, -1), [-1, 2), [2,$\\infty$)\n",
    "\n",
    "feature 2: [-$\\infty$, 5), [5,$\\infty$)\n",
    "\n",
    "\n",
    "feature 3:  [-$\\infty$, 14), [14,$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [2., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.transform(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  5., 15.],\n",
       "       [ 0.,  6., 14.],\n",
       "       [ 6.,  3., 11.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First feature: -3=>0, 0=>1, 6=>2\n",
    "Second feature: 5=>1, 6=>1, 3=>0\n",
    "Third feature: 15=>1, 14=>1, 11=>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KBinsDiscretizer implements different binning strategies, which can be selected with the strategy parameter. \n",
    "\n",
    ">  The ‘uniform’ strategy uses constant-width bins. \n",
    "\n",
    "> The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature.\n",
    "\n",
    "> The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')\n",
    " Strategy used to define the widths of the bins.\n",
    "\n",
    "* uniform: All bins in each feature have identical widths.\n",
    "* quantile： All bins in each feature have the same number of points.\n",
    "* kmeans：Values in each bin have the same nearest center of a 1D k-means cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KBinsDiscretizer in module sklearn.preprocessing._discretization:\n",
      "\n",
      "class KBinsDiscretizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', dtype=None)\n",
      " |  \n",
      " |  Bin continuous data into intervals.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_discretization>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_bins : int or array-like of shape (n_features,), default=5\n",
      " |      The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n",
      " |  \n",
      " |  encode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'\n",
      " |      Method used to encode the transformed result.\n",
      " |  \n",
      " |      onehot\n",
      " |          Encode the transformed result with one-hot encoding\n",
      " |          and return a sparse matrix. Ignored features are always\n",
      " |          stacked to the right.\n",
      " |      onehot-dense\n",
      " |          Encode the transformed result with one-hot encoding\n",
      " |          and return a dense array. Ignored features are always\n",
      " |          stacked to the right.\n",
      " |      ordinal\n",
      " |          Return the bin identifier encoded as an integer value.\n",
      " |  \n",
      " |  strategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'\n",
      " |      Strategy used to define the widths of the bins.\n",
      " |  \n",
      " |      uniform\n",
      " |          All bins in each feature have identical widths.\n",
      " |      quantile\n",
      " |          All bins in each feature have the same number of points.\n",
      " |      kmeans\n",
      " |          Values in each bin have the same nearest center of a 1D k-means\n",
      " |          cluster.\n",
      " |  \n",
      " |  dtype : {np.float32, np.float64}, default=None\n",
      " |      The desired data-type for the output. If None, output dtype is\n",
      " |      consistent with input dtype. Only np.float32 and np.float64 are\n",
      " |      supported.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_bins_ : ndarray of shape (n_features,), dtype=np.int_\n",
      " |      Number of bins per feature. Bins whose width are too small\n",
      " |      (i.e., <= 1e-8) are removed with a warning.\n",
      " |  \n",
      " |  bin_edges_ : ndarray of ndarray of shape (n_features,)\n",
      " |      The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n",
      " |      Ignored features will have empty arrays.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  Binarizer : Class used to bin values as ``0`` or\n",
      " |      ``1`` based on a parameter ``threshold``.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  In bin edges for feature ``i``, the first and last values are used only for\n",
      " |  ``inverse_transform``. During transform, bin edges are extended to::\n",
      " |  \n",
      " |    np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n",
      " |  \n",
      " |  You can combine ``KBinsDiscretizer`` with\n",
      " |  :class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess\n",
      " |  part of the features.\n",
      " |  \n",
      " |  ``KBinsDiscretizer`` might produce constant features (e.g., when\n",
      " |  ``encode = 'onehot'`` and certain bins do not contain any data).\n",
      " |  These features can be removed with feature selection algorithms\n",
      " |  (e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[-2, 1, -4,   -1],\n",
      " |  ...      [-1, 2, -3, -0.5],\n",
      " |  ...      [ 0, 3, -2,  0.5],\n",
      " |  ...      [ 1, 4, -1,    2]]\n",
      " |  >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
      " |  >>> est.fit(X)\n",
      " |  KBinsDiscretizer(...)\n",
      " |  >>> Xt = est.transform(X)\n",
      " |  >>> Xt  # doctest: +SKIP\n",
      " |  array([[ 0., 0., 0., 0.],\n",
      " |         [ 1., 1., 1., 0.],\n",
      " |         [ 2., 2., 2., 1.],\n",
      " |         [ 2., 2., 2., 2.]])\n",
      " |  \n",
      " |  Sometimes it may be useful to convert the data back into the original\n",
      " |  feature space. The ``inverse_transform`` function converts the binned\n",
      " |  data into the original feature space. Each value will be equal to the mean\n",
      " |  of the two bin edges.\n",
      " |  \n",
      " |  >>> est.bin_edges_[0]\n",
      " |  array([-2., -1.,  0.,  1.])\n",
      " |  >>> est.inverse_transform(Xt)\n",
      " |  array([[-1.5,  1.5, -3.5, -0.5],\n",
      " |         [-0.5,  2.5, -2.5, -0.5],\n",
      " |         [ 0.5,  3.5, -1.5,  0.5],\n",
      " |         [ 0.5,  3.5, -1.5,  1.5]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KBinsDiscretizer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_bins=5, *, encode='onehot', strategy='quantile', dtype=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Data to be discretized.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Transform discretized data back to original feature space.\n",
      " |      \n",
      " |      Note that this function does not regenerate the original data\n",
      " |      due to discretization rounding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data in the binned space.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xinv : ndarray, dtype={np.float32, np.float64}\n",
      " |          Data in the original feature space.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Discretize the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Data to be discretized.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : {ndarray, sparse matrix}, dtype={np.float32, np.float64}\n",
      " |          Data in the binned space. Will be a sparse matrix if\n",
      " |          `self.encode='onehot'` and ndarray otherwise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " help(preprocessing.KBinsDiscretizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
