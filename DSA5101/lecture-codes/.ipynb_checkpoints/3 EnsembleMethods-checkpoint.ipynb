{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA5101 - Introduction to Big Data for Industry\n",
    "\n",
    "\n",
    "**Prepared by *Dr Li Xiaoli*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Ensemble methods     \n",
    "\n",
    "\n",
    "##  The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability/robustness over a single estimator.\n",
    "\n",
    "## Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "> In averaging methods, the driving principle is to build several estimators *independently* and then to average their predictions. On average, the combined estimator could be better than any of the single base estimator because its variance is reduced. Examples: Bagging methods, Random Forest Classifier (special cases of bagging, Forests of randomized trees).\n",
    "\n",
    "> By contrast, in boosting methods, base estimators are built *sequentially* and one tries to reduce the bias of the combined estimator. Later estimators tried to correct the unaddressed errors left by previous estimators. The motivation is to combine several weak models to produce a powerful ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random Forest (and Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier (Forests of randomized trees) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)  \n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "> **n_estimators** : integer, optional (default=10)\n",
    "    *The number of trees in the forest.*\n",
    " \n",
    "> **criterion** : string, optional (default=\"gini\")\n",
    "\n",
    "  > The function to *measure the quality of a split. Supported criteria are\n",
    " \"gini\" for the Gini impurity and \"entropy\" for the information gain.*\n",
    " \n",
    "> **max_features** : int, float, string or None, optional (default=\"auto\").\n",
    "  **The number of features to consider when looking for the best split:**\n",
    ">> * If int, then consider `max_features` features at each split.\n",
    ">> * If float, then `max_features` is a percentage/ratio and\n",
    "  `int(max_features * n_features)` features are considered at each split.\n",
    ">> * If \"auto\", then `max_features=sqrt(n_features)`.\n",
    ">> * If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    ">> * If \"log2\", then `max_features=log2(n_features)`.\n",
    ">> * If None, then `max_features=n_features`.\n",
    "\n",
    "\n",
    "> **max_depth**: integer or None, optional (default=None)\n",
    "\n",
    "> The maximum depth of the tree. If None, then nodes are expanded until\n",
    "   all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "   \n",
    ">  min_samples_split : int, float, optional (default=2)\n",
    "\n",
    "> The minimum number of samples required to split an internal node:\n",
    ">>..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly generate 10 points/examples that will be equally divided among \n",
    "#3 clusters. Each point/example has 2 features, i.e. 10 2-D vectors. \n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10, centers=3, n_features=2, random_state=0)\n",
    "\n",
    "# X : array of shape [n_samples, n_features]\n",
    "# The generated samples.\n",
    "    \n",
    "# y : array of shape [n_samples]\n",
    "# The integer labels for cluster membership of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help (make_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12031365,  5.75806083],\n",
       "       [ 1.7373078 ,  4.42546234],\n",
       "       [ 2.36833522,  0.04356792],\n",
       "       [ 0.87305123,  4.71438583],\n",
       "       [-0.66246781,  2.17571724],\n",
       "       [ 0.74285061,  1.46351659],\n",
       "       [-4.07989383,  3.57150086],\n",
       "       [ 3.54934659,  0.6925054 ],\n",
       "       [ 2.49913075,  1.23133799],\n",
       "       [ 1.9263585 ,  4.15243012]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "# 10 points that will be equally divided among 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n",
    "# 3 labels: 0, 1, 2\n",
    "# We are solving a three class classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###       Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will build Decision Tree models\n",
      "0.9823000000000001\n"
     ]
    }
   ],
   "source": [
    "print (\"We will build Decision Tree models\")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# samples_generator: generate training data that consist of 10,000 examples \n",
    "# from 100 clusters, each example is reresented as 10-d vectors\n",
    "# VERY COMVINIENT!\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,random_state=0)\n",
    "\n",
    "# DecisionTreeClassifier \n",
    "# max_depth: The maximum depth of the tree.\n",
    "# max_depth=None: nodes are expanded until all leaves are pure \n",
    "# or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "# min_samples_split: The minimum number of samples required to split \n",
    "# an internal node.\n",
    "# min_samples_split=1, we split the internal node until it has 1 sample\n",
    "\n",
    "# Build DT classifier, with the maximum depth, and split unitl contain 1 sample if not pure\n",
    "\n",
    "# min_samples_split : int, float, optional (default=2)\n",
    "# The minimum number of samples required to split an internal node (its value is >=2):\n",
    "#  - If int, then consider `min_samples_split` as the minimum number. \n",
    "#  - If float, then `min_samples_split` is a percentage and \n",
    "# ceil(min_samples_split * n_samples) is the minimum number of samples \n",
    "# for each split.\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)  \n",
    "#clf = DecisionTreeClassifier(max_depth=None, min_samples_split=3,random_state=0) \n",
    "#clf = DecisionTreeClassifier(max_depth=None,random_state=0)  \n",
    "scores = cross_val_score(clf, X, y) #compute the performance of DT classifier\n",
    "print (scores.mean())                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###       Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will build Random Forests models\n",
      "0.9997\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "print (\"We will build Random Forests models\")\n",
    "# The number of trees in the forest: 10\n",
    "# nodes are expanded until all leaves are pure \n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n",
    "RFscores = cross_val_score(clf, X, y)\n",
    "print (RFscores.mean())                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997999999999999\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "#       Random Forests  - You can change max_depth to see effect    #\n",
    "#####################################################################\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=15, random_state=0)\n",
    "RFscores = cross_val_score(clf, X, y) # default 5-fold cross validation\n",
    "print (RFscores.mean())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? cross_val_score \n",
    "#Evaluate a score by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "#       Random Forests  - You can change number of trees    #\n",
    "#############################################################\n",
    "clf = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=0)\n",
    "RFscores = cross_val_score(clf, X, y)\n",
    "print (RFscores.mean())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example shows Random Forests can perform better than decision tree model.\n",
    "### More trees typically lead to better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "0.9466666666666665\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "#               AdaBoost                #\n",
    "#########################################\n",
    "# https://en.wikipedia.org/wiki/AdaBoost\n",
    "print (\"AdaBoost\")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=500)  \n",
    "# Build AdaBoost classier with 100 base classifiers\n",
    "scores = cross_val_score(clf, iris.data, iris.target)\n",
    "print(scores.mean())                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Tree Boosting for Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Tree Boosting for Classification\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "#   Gradient Tree Boosting for Classification       #\n",
    "#####################################################\n",
    "print (\"Gradient Tree Boosting for Classification\")\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates data for binary classification used in Hastie et al. 2009, \n",
    "# Example 10.2. # by default, we generate 12,000 data points (each with 10 features), \n",
    "# with binary label 1 and -1\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# n_samples : int, optional (default=12000)\n",
    "\n",
    "#    Returns\n",
    "#    -------\n",
    "#    X : array of shape [n_samples, 10]\n",
    "#        The input samples.\n",
    "    \n",
    "#    y : array of shape [n_samples]\n",
    "#        The output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help (make_hastie_10_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721,  0.97873798, ..., -0.15135721,\n",
       "        -0.10321885,  0.4105985 ],\n",
       "       [ 0.14404357,  1.45427351,  0.76103773, ..., -0.20515826,\n",
       "         0.3130677 , -0.85409574],\n",
       "       [-2.55298982,  0.6536186 ,  0.8644362 , ..., -0.18718385,\n",
       "         1.53277921,  1.46935877],\n",
       "       ...,\n",
       "       [ 0.19986465,  0.26134578, -0.1279868 , ..., -0.51718289,\n",
       "         0.07969414,  1.01612661],\n",
       "       [-0.15167316, -1.42519962,  1.07092211, ..., -1.20676602,\n",
       "        -1.04746487,  0.0075881 ],\n",
       "       [-0.09708998,  0.78044425,  0.22108152, ...,  2.53170549,\n",
       "        -0.03572203,  0.17320019]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  1., ..., -1., -1.,  1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "# We generate 12000 examples, each with 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "# We generate 12000 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment data into training and testing\n",
    "X_train, X_test = X[:2000], X[2000:] #Data: First 2000 are training data, while last 10000 for test data\n",
    "\n",
    "# X_train.shape=(2000, 10):2000 examples, each with 10 features\n",
    "# X_test.shape=(10000, 10): 10000 examples, each with 10 features \n",
    "\n",
    "y_train, y_test = y[:2000], y[2000:] #Label: First 2000 are training labels, while last 10000 for test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=10, random_state=0).fit(X_train, y_train)\n",
    "print (clf.score(X_test, y_test))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Tree Boosting for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Tree Boosting for Regression\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "#       Gradient Tree Boosting for Regression       #\n",
    "#####################################################\n",
    "print (\"Gradient Tree Boosting for Regression\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0) #generate 1200 example, with each 10 features\n",
    "X_train, X_test = X[:200], X[200:]  # first 200 as training\n",
    "y_train, y_test = y[:200], y[200:]  # 201-1200 as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help (make_friedman1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.40631483, 19.60677754, 14.74407804, ..., 11.46321759,\n",
       "        6.49856896, 20.32981295])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  #y are continous/numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.009154859960321\n"
     ]
    }
   ],
   "source": [
    "#est is our regression model\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "#performance in test set in terms of mean squared error\n",
    "print (mean_squared_error(y_test, est.predict(X_test)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8404283706219466"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8058912110043196"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature importance scores for 10 features\n",
      "[0.10684213 0.10461707 0.11265447 0.09863589 0.09469133 0.10729306\n",
      " 0.09163753 0.09718194 0.09581415 0.09063242]\n"
     ]
    }
   ],
   "source": [
    "# Classifier, which features are important\n",
    "#The feature importance scores of a fit gradient boosting model \n",
    "#can be accessed via the feature_importances_ property:\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X, y)\n",
    "print (\"The feature importance scores for 10 features\")\n",
    "print (clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier\n",
      "Majority Class Labels (Majority/Hard Voting)\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#               VotingClassifier                #\n",
    "#################################################\n",
    "print (\"VotingClassifier\")\n",
    "\n",
    "###############################################\n",
    "# Majority Class Labels (Majority/Hard Voting)\n",
    "print (\"Majority Class Labels (Majority/Hard Voting)\")\n",
    "from sklearn import datasets\n",
    "#from sklearn.metrics import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression   # classifier 1\n",
    "from sklearn.naive_bayes import GaussianNB            # classifier 2       \n",
    "from sklearn.ensemble import RandomForestClassifier   # classifier 3\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.953 (+/- 0.0400) [Logistic Regression]\n",
      "Accuracy: 0.940 (+/- 0.0389) [Random Forest]\n",
      "Accuracy: 0.913 (+/- 0.0400) [naive Bayes]\n",
      "Accuracy: 0.953 (+/- 0.0400) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "# Build 3 individual classifiers \n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "# Voting-ensemble classifier (4th classifier)\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "#Loop for 4 classifiers with their corresponding labels\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.3f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change %0.2f to %0.3f and see effect"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
